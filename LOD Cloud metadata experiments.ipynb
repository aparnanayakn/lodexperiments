{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import json\n",
    "import sys\n",
    "from rdflib import Graph\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLODData():\n",
    "    url = \"http://lod-cloud.net/lod-data.json\"\n",
    "    response = urllib2.urlopen(url)\n",
    "    data = json.loads(response.read())\n",
    "    return data\n",
    "\n",
    "lod_data = loadLODData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the different Media Types used within the LOD Cloud dataset metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import Graph\n",
    "from rdflib.plugin import register, Serializer, Parser\n",
    "\n",
    "register('rdfa', Parser,'rdflib.plugins.parsers.rdfa', 'RDFaParser')\n",
    "\n",
    "\n",
    "rdfaCounter = 0\n",
    "counter = 0\n",
    "\n",
    "# Different Media Types Used\n",
    "def __extractMediaType(item):\n",
    "    global rdfaCounter\n",
    "    global counter\n",
    "    mtype = item[\"media_type\"]\n",
    "    if (\";\" in mtype):\n",
    "        mtype = mtype[:mtype.find(';')]\n",
    "    mtype = str(mtype)\n",
    "    if mtype == \"text/html\":\n",
    "        if \"access_url\" in item:\n",
    "            try:\n",
    "                h = urllib2.urlopen(str(item[\"access_url\"]).strip(), timeout=10)\n",
    "                if (h.getcode() > 399):\n",
    "                    g = Graph()\n",
    "                    g.parse(str(item[\"access_url\"]).strip(), 'rdfa')\n",
    "                    if (len(g) > 1): \n",
    "                        rdfaCounter = rdfaCounter + 1\n",
    "            except :\n",
    "                pass\n",
    "    if (len(mtype) == 0):\n",
    "            mtype = \"None\"\n",
    "    if (mtype in mediaTypes):\n",
    "        mediaTypes[mtype] = mediaTypes[mtype] + 1\n",
    "    else:\n",
    "        mediaTypes[mtype] = 1\n",
    "        \n",
    "\n",
    "mediaTypes = dict({})\n",
    "# Get Different Media Types\n",
    "for key in lod_data:\n",
    "    full_download = lod_data[key][\"full_download\"]\n",
    "    other_download = lod_data[key][\"other_download\"]\n",
    "\n",
    "    if (len(full_download) > 0):\n",
    "            for item in full_download:\n",
    "                if (\"media_type\" in item):\n",
    "                    __extractMediaType(item)\n",
    "    if (len(other_download) > 0):\n",
    "            for item in other_download:\n",
    "                if (\"media_type\" in item):\n",
    "                    __extractMediaType(item)\n",
    "\n",
    "# Create Chart                                     \n",
    "toPlot = dict({})\n",
    "toPlot[\"Others\"] = 0\n",
    "\n",
    "for (k,v) in mediaTypes.items():\n",
    "    if k == \"text/html\":\n",
    "#         skip text/html as it would be large to print\n",
    "        continue\n",
    "    if (v > 25):\n",
    "        toPlot[k] = v\n",
    "    else:\n",
    "        toPlot[\"Others\"] = toPlot[\"Others\"] + 1\n",
    "\n",
    "\n",
    "# Display Table\n",
    "print(\"\\033[4mTabular View\\033[0m\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "toPlot[\"text/html\"] = mediaTypes[\"text/html\"]\n",
    "sorted_toPlot = sorted(toPlot.items(), key=lambda kv: kv[1])\n",
    "print \"{:<50} {:<10}\".format('\\033[1m' +'Media Type','Frequency'+'\\033[0m')\n",
    "for k, v in sorted_toPlot:\n",
    "    print \"{:<50} {:<10}\".format(k, v)\n",
    "    \n",
    "print \"Total text/html in RDFa: \"+ str(rdfaCounter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the Dataset's Accessibility based on the LOD Cloud available metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptable_media_types = set()\n",
    "acceptable_media_types.add(\"application/x-ntriples\")\n",
    "acceptable_media_types.add(\"application/rdf+xml\")\n",
    "acceptable_media_types.add(\"text/turtle\")\n",
    "acceptable_media_types.add(\"application/x-nquads\")\n",
    "acceptable_media_types.add(\"application/trig\")\n",
    "acceptable_media_types.add(\"application/n-triples\")\n",
    "acceptable_media_types.add(\"gzip:ntriples\")\n",
    "acceptable_media_types.add(\"application/x-gzip\")\n",
    "acceptable_media_types.add(\"application/octet-stream\")\n",
    "acceptable_media_types.add(\"application/x-ntriples\")\n",
    "acceptable_media_types.add(\"RDF\")\n",
    "acceptable_media_types.add(\"plain/text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON, XML\n",
    "from SPARQLWrapper.SPARQLExceptions import EndPointInternalError\n",
    "from rdflib.plugin import register, Serializer, Parser\n",
    "\n",
    "def __query_endpoint(uri):\n",
    "    try:\n",
    "        sparql = SPARQLWrapper(uri)\n",
    "        sparql.setQuery('ASK {?s ?p ?o}')\n",
    "        sparql.setReturnFormat(XML)\n",
    "        sparql.setTimeout(3)\n",
    "        results = sparql.query().convert()        \n",
    "        for result in results.getElementsByTagName('boolean'):\n",
    "            return True\n",
    "        return False \n",
    "    except (EndPointInternalError, AttributeError) as epex:\n",
    "        try:\n",
    "            params = urllib.urlencode({'query': 'ASK {?s ?p ?o}'})\n",
    "            opener = urllib2.build_opener(urllib2.HTTPHandler)\n",
    "            request = urllib2.Request(uri+'?'+params)\n",
    "            request.get_method = lambda: 'GET'\n",
    "            request.add_header('Accept', 'application/sparql-results+json')\n",
    "            url = opener.open(request, timeout=3)\n",
    "            data = url.read()\n",
    "            results = json.loads(data)\n",
    "            if(results['boolean'] is not None):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        except Exception as noContentNego:\n",
    "            try:\n",
    "                params = urllib.urlencode({'query': 'ASK {?s ?p ?o}'})\n",
    "                opener = urllib2.build_opener(urllib2.HTTPHandler)\n",
    "                request = urllib2.Request(uri+'?'+params)\n",
    "                request.get_method = lambda: 'GET'\n",
    "                url = opener.open(request, timeout=3)\n",
    "                data = url.read()\n",
    "                if (data):\n",
    "                    return True\n",
    "                else: \n",
    "                    return False\n",
    "            except:\n",
    "                e = sys.exc_info()[0]\n",
    "                return False\n",
    "    except:\n",
    "        e = sys.exc_info()[0]\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __query_void(url, voidurl):\n",
    "    if (__checkstatus(voidurl)):\n",
    "        try:\n",
    "            graph = rdflib.Graph()\n",
    "            graph.parse(voidurl)\n",
    "            accessible = False\n",
    "            \n",
    "            result = graph.query('ASK { ?s a <http://rdfs.org/ns/void#Dataset> . }')\n",
    "            for row in result:\n",
    "                accessible = bool(row) \n",
    "         \n",
    "            return accessible\n",
    "        except:\n",
    "            e = sys.exc_info()[0]\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __checkstatus(host):\n",
    "    try:\n",
    "        h = urllib2.urlopen(host, timeout=10)\n",
    "        if (h.getcode() > 399):\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def availableSPARQLEntryPoint(record):\n",
    "    sparqlEndpoint = None\n",
    "    if (\"sparql\" in record):\n",
    "        if (len(record[\"sparql\"]) >= 1):\n",
    "            sparqlEndpoint = record[\"sparql\"][0][\"access_url\"]\n",
    "    \n",
    "            if (not(\"FAIL\" in record[\"sparql\"][0][\"status\"])):\n",
    "                if (__checkstatus(sparqlEndpoint)):\n",
    "                    return __query_endpoint(sparqlEndpoint)\n",
    "                else:\n",
    "                    return False\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "\n",
    "\n",
    "def availableDatadumpEntryPoint(record):\n",
    "    datadumpLocation = []\n",
    "    full_download = record[\"full_download\"]\n",
    "    other_download = record[\"other_download\"]\n",
    "    if (len(full_download) > 0):\n",
    "            for item in full_download:\n",
    "                if (\"media_type\" in item):\n",
    "                    mtype = item[\"media_type\"]\n",
    "                    if (\";\" in mtype):\n",
    "                        mtype = mtype[:mtype.find(';')]\n",
    "                    if (item[\"media_type\"] in acceptable_media_types):\n",
    "                        if (not(\".well-known/\" in item[\"download_url\"])):\n",
    "                            if (__checkstatus(item[\"download_url\"])):\n",
    "                                datadumpLocation.append(item[\"download_url\"])\n",
    "    elif (len(other_download) > 0):\n",
    "            for item in other_download:\n",
    "                    if (not(\".well-known/\" in item[\"access_url\"])):\n",
    "                        if (item[\"media_type\"] in acceptable_media_types):\n",
    "                            if (__checkstatus(item[\"access_url\"])):\n",
    "                                datadumpLocation.append(item[\"access_url\"])\n",
    "\n",
    "    return len(datadumpLocation) > 0\n",
    "\n",
    "\n",
    "def availableVoidEntryPoint(record):\n",
    "    voidLocation = []\n",
    "    full_download = record[\"full_download\"]\n",
    "    other_download = record[\"other_download\"]\n",
    "    if (len(full_download) > 0):\n",
    "            for item in full_download:\n",
    "                if (\"media_type\" in item):\n",
    "                    mtype = item[\"media_type\"]\n",
    "                    if (\";\" in mtype):\n",
    "                        mtype = mtype[:mtype.find(';')]\n",
    "                    if (item[\"media_type\"] == \"meta/void\"):\n",
    "                        if (__checkstatus(item[\"download_url\"])):\n",
    "                            voidLocation.append(item[\"download_url\"])\n",
    "                    elif (\".well-known/\" in item[\"download_url\"]):\n",
    "                        if (__checkstatus(item[\"download_url\"])):\n",
    "                            voidLocation.append(item[\"download_url\"])\n",
    "                            \n",
    "    elif (len(other_download) > 0):\n",
    "            for item in other_download:\n",
    "                if (item[\"media_type\"] == \"meta/void\"):\n",
    "                    if (__checkstatus(item[\"access_url\"])):\n",
    "                        voidLocation.append(item[\"access_url\"])\n",
    "                elif (\".well-known/\" in item[\"access_url\"]):\n",
    "                        voidLocation.append(item[\"access_url\"])\n",
    "\n",
    "\n",
    "    return len(voidLocation) > 0\n",
    "\n",
    "\n",
    "\n",
    "dataSources = dict({}) # key, (dd,sparql,void) 1 = available 0 = not available\n",
    "for key in lod_data:\n",
    "    dataSources[key] = (availableDatadumpEntryPoint(lod_data[key]),availableSPARQLEntryPoint(lod_data[key]), availableVoidEntryPoint(lod_data[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next snippet checks the number of datasets that have no access point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noAccessPoint = 0\n",
    "\n",
    "for key in lod_data:\n",
    "    full_download = lod_data[key][\"full_download\"]\n",
    "    other_download = lod_data[key][\"other_download\"]\n",
    "    sparql = lod_data[key][\"sparql\"]\n",
    "\n",
    "    if ((len(full_download) == 0) and (len(other_download) == 0) and (len(sparql) == 0)):\n",
    "        noAccessPoint = noAccessPoint + 1\n",
    "        \n",
    "print \"Number of datasets without an access point: \"+str(noAccessPoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet will create a JSON file that can be used to recreate the LOD cloud visualisation. The LOD cloud diagram code can be found here: https://github.com/lod-cloud/lod-cloud-draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "available_lod_data = loadLODData()\n",
    "\n",
    "modified_dataSources = dict(dataSources)\n",
    "for (k,v) in dataSources.items():\n",
    "    if v == (0,0,0):\n",
    "        del modified_dataSources[k]\n",
    "        del available_lod_data[k]\n",
    "        \n",
    "print json.dumps(available_lod_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet will identify the different access points of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = 0\n",
    "sparql = 0\n",
    "void = 0\n",
    "ddSparql = 0\n",
    "ddVoid = 0\n",
    "SparqlVoid = 0\n",
    "allthree = 0\n",
    "nothing = 0\n",
    "for (k,v) in dataSources.items():\n",
    "    if v == (0,0,0):\n",
    "        nothing+=1\n",
    "    if v == (1,0,0):\n",
    "        dd+=1\n",
    "    if v == (0,1,0):\n",
    "        sparql+=1\n",
    "    if v == (0,0,1):\n",
    "        void+=1\n",
    "    if v == (1,1,0):\n",
    "        ddSparql+=1\n",
    "    if v == (1,0,1):\n",
    "        ddVoid+=1\n",
    "    if v == (0,1,1):\n",
    "        SparqlVoid+=1\n",
    "    if v == (1,1,1):\n",
    "        allthree+=1       \n",
    "        \n",
    "print(\"Only Datadump: \"+ str(dd))\n",
    "print(\"Only SPARQL: \"+ str(sparql))\n",
    "print(\"Only VOID: \"+ str(void))\n",
    "print(\"Only Datadump and SPARQL: \"+ str(ddSparql))\n",
    "print(\"Only Datadump and VOID: \"+ str(ddVoid))\n",
    "print(\"Only SPARQL and VOID: \"+ str(SparqlVoid))\n",
    "print(\"All three entry points: \"+ str(allthree))\n",
    "print(\"No entry points: \"+ str(nothing))\n",
    "\n",
    "\n",
    "labels = 'Datadump', 'SPARQL', 'voID', 'More than 1\\nDiscoverability\\nEntry', 'None'\n",
    "sizes = [dd, sparql, void, (ddSparql + ddVoid + SparqlVoid + allthree), nothing]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (7,7))\n",
    "patches, texts, autotexts = ax.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=False, startangle=90,  textprops={'fontsize': 12})\n",
    "texts[0].set_fontsize(15)\n",
    "texts[1].set_fontsize(15)\n",
    "texts[2].set_fontsize(15)\n",
    "texts[3].set_fontsize(15)\n",
    "texts[4].set_fontsize(15)\n",
    "\n",
    "ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Licenses\n",
    "\n",
    "Licences are the heart of Open Data. They define whether third parties can re-use data or otherwise, and to what extent. For this experiment we parsed through each dataset in the data file and looked for the value attributed to the `license` key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet parses through the LOD Cloud data and checks the license for each dataset. The variable `licensesUsed` stores all licenses used and their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licensesUsed = dict({})\n",
    "for key in lod_data:\n",
    "    if (\"license\" in lod_data[key]):\n",
    "        theLicence = lod_data[key][\"license\"]\n",
    "        if theLicence == \"\":\n",
    "            continue\n",
    "        if not(theLicence in licensesUsed):\n",
    "            licensesUsed[theLicence] = 1\n",
    "        else:\n",
    "            licensesUsed[theLicence] = licensesUsed[theLicence] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the licenses used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class used to visualse the data\n",
    "def visualiseLicenseData(licensesUsed):\n",
    "    fig, ax = plt.subplots(figsize = (10,8))\n",
    "\n",
    "    ax.barh(licensesUsed.keys(), licensesUsed.values(), height=0.5)\n",
    "    ax.set_yticks(np.arange(len(licensesUsed.keys())))\n",
    "    ax.set_yticklabels(licensesUsed.keys(), fontsize=12, fontweight='bold')\n",
    "    ax.invert_yaxis() \n",
    "    plt.xlabel(\"Frequency\", fontsize=15, fontweight='bold')\n",
    "\n",
    "\n",
    "    for i, v in enumerate(licensesUsed.values()):\n",
    "        ax.text(v + 2, i + 0.08 , str(v), color='black', fontweight='bold', fontsize=10)\n",
    "\n",
    "    # Display Table\n",
    "    print(\"\\033[4mTabular View\\033[0m\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    sorted_licensesUsed = sorted(licensesUsed.items(), key=lambda kv: kv[1])\n",
    "    print \"{:<60} {:<40} {:<10}\".format('\\033[1m' +'License','Frequency','Percentage'+'\\033[0m')\n",
    "    totalItems = len(lod_data)\n",
    "    for k, v in sorted_licensesUsed:\n",
    "        print \"{:<60} {:<40} {:<10}\".format(k, v, str((v * 100.0)/totalItems)+\"%\")\n",
    "    \n",
    "    # Display Plot\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"\\033[4mBar View\\033[0m\")\n",
    "    plt.show()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise all data\n",
    "visualiseLicenseData(licensesUsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Summerised Data\n",
    "licenseLabels = dict({})\n",
    "\n",
    "licenseLabels['https://creativecommons.org/licenses/by/3.0/'] = \"CC-BY-3.0\"           \n",
    "licenseLabels['http://reference.data.gov.uk/id/open-government-licence'] = \"OGL-UK\"\n",
    "licenseLabels['http://www.opendefinition.org/licenses/odc-by'] = \"ODC-BY\"        \n",
    "licenseLabels['http://www.opendefinition.org/licenses/odc-pddl'] = \"ODC-PDDL\"\n",
    "licenseLabels['http://www.opendefinition.org/licenses/odc-odbl'] = \"ODC-ODBL\"                           \n",
    "licenseLabels['http://creativecommons.org/licenses/by-nc/2.0/'] = \"CC-BY-NC-2.0\"\n",
    "licenseLabels['http://www.opendefinition.org/licenses/cc-zero'] = \"CC0\"\n",
    "licenseLabels['http://www.opendefinition.org/licenses/cc-by-sa'] = \"CC-BY-SA\"              \n",
    "licenseLabels['http://www.opendefinition.org/licenses/cc-by'] = \"CC-BY\"\n",
    "\n",
    "summLicensesUsed = dict({})\n",
    "summLicensesUsed['Other'] = 0\n",
    "for k,v in licensesUsed.items():\n",
    "    if (v < 10):\n",
    "        summLicensesUsed['Other'] = summLicensesUsed['Other'] + v\n",
    "    else:\n",
    "        if k in licenseLabels:\n",
    "            summLicensesUsed[licenseLabels[k]] = v\n",
    "\n",
    "visualiseLicenseData(summLicensesUsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next experiment, we use a regular expression to identify the dataset which potentially have a license assigned to its description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def __tryDecoding(text):\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "        return text\n",
    "    except TypeError:\n",
    "        return text\n",
    "    \n",
    "def __licenseStringExtractor(text):\n",
    "    potentialText = __tryDecoding(text)\n",
    "    \n",
    "    if (potentialText):\n",
    "        str_list = potentialText.splitlines()\n",
    "        str_list = filter(None, str_list)\n",
    "        new_desc = ' '.join([__tryDecoding(x) for x in str_list])\n",
    "\n",
    "        p = re.compile(r'.*(licensed?|copyrighte?d?).*(under|grante?d?|rights?).*',re.IGNORECASE | re.MULTILINE)\n",
    "        \n",
    "        m = p.match(new_desc)\n",
    "\n",
    "        return not(m == None)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "potentialLicence = []\n",
    "for key in lod_data:\n",
    "    if (\"description\" in lod_data[key]):\n",
    "        text = lod_data[key][\"description\"]['en']\n",
    "        if (__licenseStringExtractor(text)):\n",
    "            potentialLicence.append(str(key))\n",
    "\n",
    "print(potentialLicence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
